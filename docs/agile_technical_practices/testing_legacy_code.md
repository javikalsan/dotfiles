# Testing legacy code

*"Code without tests is bad code. It doesn't matter how well written it is;
it doesn't matter how pretty or object-oriented or well-encapsulated it is.
With tests, we can change the behavior of our code quickly and verifiably.
Without them, we really don't know if our code is getting better or worse.
To me, legacy code is simply code without tests."*

Michael C. Feathers, Working Effectively with Legacy Code

## Breaking dependencies using a seam

* Break the dependency to allow the Subject Under Test to be exercised independently.
* Replace the dependency with a Test Double.
* For static dependencies or locally generated ones you will have to segregate the dependency,
introducing a seam in your code: find soft points where we can separate coupled parts.
If your language supports passing functions as parameters, you can use the [Peel and Slice technique](https://design-zen.blogspot.com/2014/11/unit-testing-peel-technique.html).

## Use inheritance to decouple production code
* Extract to method to encapsulate the behavior.
* In the test we can create a new class inheriting and overriding to control the returned value.
* We can use the above technique for static methods.
* üëç minimizes changes to production code.
* üëç minimizes minimizing the risk of introducing breaking changes to production code.

## Golden Master

### Feasibility
1. Does the system have clear inputs and outputs?
2. Does the system generate the same output for the same input? If not, can we use a Test Double to make it so? Isolate the side effects.
3. Can we capture the output of the system on our tests without changing the behavior of the system?
The options are to either redirect the output to a file or replace the data layer with an in-memory data layer.
4. Can we inject the input of the system on our tests without changing the behavior of the system? Same options as point 3.

### Generate input/output
1. Gradually create a fake input for the system and persist it on a file.
2. Create a test that loads the fake input, injects it into the system, and captures and persists the output.
3. Measure the test coverage. Repeat until coverage is close to or at 100%.

### Assert
1. Expand the previous test to assert that given the input, we get the expected output saved from the previous step.
2. Commit. Don‚Äôt forget to include the Golden Master input and output files.
3. Experiment with small mutations on the production code to check that the test fails. Revert when done.
If you're not pleased with the results, go back to the previous section and generate better input/output combinations,
or write new tests with new input/output combinations.


## Characterization tests
* Protect the existing behavior via automated testing.
1. Use a piece of code in a test harness.
2. Write an assertion that you know will fail.
3. Run the test and let the failure tell you what the actual behavior is.
4. Change the test so that it expects the behavior that the code actually produces.
5. Repeat until you are reasonably sure all the degrees of freedom are identified and tested.
6. Name the test according to the business behavior you are characterizing.

## Approval Tests by Llewellyn Falco
* Based on the idea of a Golden Master.
* API that allows for the definition of scenarios and verification of outputs.
* Use code coverage to ensure you are creating all necessary tests to refactor with safety after.

## Combination Tests
* A feature of approval tests.
* Given a known set of inputs, they will be combined in all possible ways to exercise as many execution paths as possible.
* ‚ö† The resulting collection of parameters could increase so quickly.

## Revisiting the refactoring guidelines
**Stay in the green while refactoring**
Don't change production code that is not covered by tests. If we need to refactor some code that has no tests,
then start by adding behavior tests.
